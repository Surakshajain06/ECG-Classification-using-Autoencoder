{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25d5a196",
   "metadata": {},
   "source": [
    "Load original labels and save encoded labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8cc4248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded labels saved to: ecg_labels_encoded.npy\n",
      "Original labels saved to: ecg_original_labels.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def encode_and_save_labels(\n",
    "    input_npz=\"../src/Preprocessed_data/ecg_data.npz\",\n",
    "    encoded_labels_path=\"ecg_labels_encoded.npy\",\n",
    "    original_labels_path=\"ecg_original_labels.npy\"\n",
    "):\n",
    "    # Load original labels from your npz data file\n",
    "    data = np.load(input_npz)\n",
    "    labels = data['labels']  # shape: (num_beats,)\n",
    "\n",
    "    # Encode string labels to integers\n",
    "    le = LabelEncoder()\n",
    "    labels_encoded = le.fit_transform(labels)\n",
    "\n",
    "    # Save directly in current folder\n",
    "    np.save(encoded_labels_path, labels_encoded)\n",
    "    np.save(original_labels_path, labels)\n",
    "\n",
    "    print(f\"Encoded labels saved to: {encoded_labels_path}\")\n",
    "    print(f\"Original labels saved to: {original_labels_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    encode_and_save_labels()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da02bc7",
   "metadata": {},
   "source": [
    "Imports & load data (sanity checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da9faf69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw beats shape: (109487, 90, 1)\n",
      "labels shape: (109487,)\n",
      "label counts: Counter({np.int64(1): 90625, np.int64(2): 8043, np.int64(4): 7235, np.int64(3): 2781, np.int64(0): 803})\n",
      "after reshape beats shape: (109487, 1, 90)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "npz_path = \"../src/Preprocessed_data/ecg_data.npz\"  # change if needed\n",
    "encoded_labels_path = \"ecg_labels_encoded.npy\"      # should be in Train_classifier\n",
    "\n",
    "# load npz and labels\n",
    "data = np.load(npz_path)\n",
    "beats = data['beats']              # expected shapes: (N, 90) or (N, 90, 1) or (N, 1, 90)\n",
    "labels_encoded = np.load(encoded_labels_path)  # integers\n",
    "\n",
    "print(\"raw beats shape:\", beats.shape)\n",
    "print(\"labels shape:\", labels_encoded.shape)\n",
    "print(\"label counts:\", Counter(labels_encoded))\n",
    "\n",
    "# Normalize shape -> (N, 1, L) for PyTorch conv1d convention\n",
    "if beats.ndim == 2:\n",
    "    # (N, L) -> (N, 1, L)\n",
    "    beats = beats[:, np.newaxis, :]\n",
    "elif beats.ndim == 3 and beats.shape[-1] == 1:\n",
    "    # most common: (N, 90, 1) -> (N, 1, 90)\n",
    "    beats = np.transpose(beats, (0, 2, 1))\n",
    "# if it's already (N,1,L) we do nothing\n",
    "\n",
    "print(\"after reshape beats shape:\", beats.shape)\n",
    "assert beats.ndim == 3 and beats.shape[1] == 1, \"beats must be (N,1,L) after reshape\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b246f99",
   "metadata": {},
   "source": [
    "Train / validation / test split (stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8eafa598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (76640, 1, 90) (76640,)\n",
      "val:   (16423, 1, 90) (16423,)\n",
      "test:  (16424, 1, 90) (16424,)\n",
      "train label counts: Counter({np.int64(1): 63436, np.int64(2): 5630, np.int64(4): 5065, np.int64(3): 1947, np.int64(0): 562})\n",
      "val   label counts: Counter({np.int64(1): 13594, np.int64(2): 1206, np.int64(4): 1085, np.int64(3): 417, np.int64(0): 121})\n",
      "test  label counts: Counter({np.int64(1): 13595, np.int64(2): 1207, np.int64(4): 1085, np.int64(3): 417, np.int64(0): 120})\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: stratified train/val/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = beats\n",
    "y = labels_encoded\n",
    "\n",
    "# Keep a held-out test set: 15% of data\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Split trainval into train + val so val is ~15% of whole dataset as well\n",
    "val_fraction_of_trainval = 0.15 / 0.85  # â‰ˆ 0.17647\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval, test_size=val_fraction_of_trainval, stratify=y_trainval, random_state=42\n",
    ")\n",
    "\n",
    "print(\"train:\", X_train.shape, y_train.shape)\n",
    "print(\"val:  \", X_val.shape, y_val.shape)\n",
    "print(\"test: \", X_test.shape, y_test.shape)\n",
    "print(\"train label counts:\", Counter(y_train))\n",
    "print(\"val   label counts:\", Counter(y_val))\n",
    "print(\"test  label counts:\", Counter(y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc25665",
   "metadata": {},
   "source": [
    "Prepare Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0750f074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Convert numpy to torch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5459d239",
   "metadata": {},
   "source": [
    "Compute Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "676e32db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [  562 63436  5630  1947  5065]\n",
      "Class weights: [0.66300315 0.00587376 0.06618255 0.19137533 0.07356521]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Compute inverse frequency weights for CrossEntropyLoss\n",
    "class_counts = np.bincount(y_train)\n",
    "class_weights = 1. / class_counts\n",
    "class_weights = class_weights / class_weights.sum()  # normalize if needed\n",
    "\n",
    "print(\"Class counts:\", class_counts)\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "# Convert to torch tensor for the loss function\n",
    "weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102369f5",
   "metadata": {},
   "source": [
    "Create WeightedRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b63a0e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Sample weights for each instance in training data\n",
    "sample_weights = class_weights[y_train]  # numpy indexing\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=torch.tensor(sample_weights, dtype=torch.float32),\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb39f63",
   "metadata": {},
   "source": [
    "Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5adbc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ECGClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ECGClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "\n",
    "        self.fc1 = nn.Linear(32 * 22, 64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b153a372",
   "metadata": {},
   "source": [
    "Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "237adb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ECGClassifier(num_classes=len(class_counts))\n",
    "\n",
    "# Weighted CrossEntropyLoss\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0b0938",
   "metadata": {},
   "source": [
    "Training Loop with Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06085885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                outputs = model(X_batch)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                y_true.extend(y_batch.numpy())\n",
    "                y_pred.extend(preds.numpy())\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        print(f\"Train Loss: {train_loss/len(train_loader):.4f}\")\n",
    "        print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7affcd",
   "metadata": {},
   "source": [
    "Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a95a1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "Train Loss: 0.1730\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0394    0.9339    0.0757       121\n",
      "           1     0.9958    0.0524    0.0995     13594\n",
      "           2     0.8755    0.9793    0.9245      1206\n",
      "           3     0.0404    0.9640    0.0776       417\n",
      "           4     0.6006    0.8581    0.7066      1085\n",
      "\n",
      "    accuracy                         0.2033     16423\n",
      "   macro avg     0.5104    0.7575    0.3768     16423\n",
      "weighted avg     0.9296    0.2033    0.1995     16423\n",
      "\n",
      "\n",
      "Epoch 2/10\n",
      "Train Loss: 0.0750\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0460    0.9256    0.0877       121\n",
      "           1     0.9979    0.2085    0.3449     13594\n",
      "           2     0.8936    0.9892    0.9390      1206\n",
      "           3     0.0517    0.9616    0.0981       417\n",
      "           4     0.4968    0.9419    0.6505      1085\n",
      "\n",
      "    accuracy                         0.3387     16423\n",
      "   macro avg     0.4972    0.8054    0.4240     16423\n",
      "weighted avg     0.9261    0.3387    0.4006     16423\n",
      "\n",
      "\n",
      "Epoch 3/10\n",
      "Train Loss: 0.0526\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0601    0.9256    0.1129       121\n",
      "           1     0.9970    0.1704    0.2910     13594\n",
      "           2     0.9620    0.9867    0.9742      1206\n",
      "           3     0.0458    0.9808    0.0876       417\n",
      "           4     0.4897    0.9382    0.6435      1085\n",
      "\n",
      "    accuracy                         0.3072     16423\n",
      "   macro avg     0.5109    0.8004    0.4218     16423\n",
      "weighted avg     0.9298    0.3072    0.3580     16423\n",
      "\n",
      "\n",
      "Epoch 4/10\n",
      "Train Loss: 0.0424\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0554    0.9339    0.1046       121\n",
      "           1     0.9984    0.2795    0.4368     13594\n",
      "           2     0.9022    0.9942    0.9460      1206\n",
      "           3     0.0531    0.9712    0.1007       417\n",
      "           4     0.6280    0.9382    0.7524      1085\n",
      "\n",
      "    accuracy                         0.3979     16423\n",
      "   macro avg     0.5274    0.8234    0.4681     16423\n",
      "weighted avg     0.9359    0.3979    0.4840     16423\n",
      "\n",
      "\n",
      "Epoch 5/10\n",
      "Train Loss: 0.0360\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.1083    0.9091    0.1935       121\n",
      "           1     0.9984    0.4520    0.6223     13594\n",
      "           2     0.9246    0.9967    0.9593      1206\n",
      "           3     0.0662    0.9712    0.1240       417\n",
      "           4     0.5672    0.9604    0.7132      1085\n",
      "\n",
      "    accuracy                         0.5422     16423\n",
      "   macro avg     0.5329    0.8579    0.5225     16423\n",
      "weighted avg     0.9342    0.5422    0.6372     16423\n",
      "\n",
      "\n",
      "Epoch 6/10\n",
      "Train Loss: 0.0324\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0871    0.8843    0.1585       121\n",
      "           1     0.9989    0.3216    0.4866     13594\n",
      "           2     0.9330    0.9925    0.9618      1206\n",
      "           3     0.0527    0.9904    0.1001       417\n",
      "           4     0.6119    0.9576    0.7467      1085\n",
      "\n",
      "    accuracy                         0.4340     16423\n",
      "   macro avg     0.5367    0.8293    0.4907     16423\n",
      "weighted avg     0.9377    0.4340    0.5264     16423\n",
      "\n",
      "\n",
      "Epoch 7/10\n",
      "Train Loss: 0.0275\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.1131    0.9174    0.2015       121\n",
      "           1     0.9984    0.5013    0.6674     13594\n",
      "           2     0.9203    0.9959    0.9566      1206\n",
      "           3     0.0709    0.9688    0.1321       417\n",
      "           4     0.6485    0.9641    0.7754      1085\n",
      "\n",
      "    accuracy                         0.5831     16423\n",
      "   macro avg     0.5502    0.8695    0.5466     16423\n",
      "weighted avg     0.9395    0.5831    0.6788     16423\n",
      "\n",
      "\n",
      "Epoch 8/10\n",
      "Train Loss: 0.0251\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.1050    0.9008    0.1881       121\n",
      "           1     0.9985    0.5264    0.6894     13594\n",
      "           2     0.9478    0.9942    0.9705      1206\n",
      "           3     0.0740    0.9664    0.1374       417\n",
      "           4     0.7021    0.9733    0.8158      1085\n",
      "\n",
      "    accuracy                         0.6042     16423\n",
      "   macro avg     0.5655    0.8722    0.5602     16423\n",
      "weighted avg     0.9451    0.6042    0.7007     16423\n",
      "\n",
      "\n",
      "Epoch 9/10\n",
      "Train Loss: 0.0207\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.1766    0.8843    0.2944       121\n",
      "           1     0.9969    0.5923    0.7431     13594\n",
      "           2     0.9600    0.9942    0.9768      1206\n",
      "           3     0.0799    0.9544    0.1475       417\n",
      "           4     0.6967    0.9696    0.8108      1085\n",
      "\n",
      "    accuracy                         0.6581     16423\n",
      "   macro avg     0.5820    0.8790    0.5945     16423\n",
      "weighted avg     0.9450    0.6581    0.7463     16423\n",
      "\n",
      "\n",
      "Epoch 10/10\n",
      "Train Loss: 0.0226\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.1219    0.8926    0.2145       121\n",
      "           1     0.9978    0.6204    0.7651     13594\n",
      "           2     0.9747    0.9917    0.9831      1206\n",
      "           3     0.0923    0.9568    0.1684       417\n",
      "           4     0.6803    0.9631    0.7974      1085\n",
      "\n",
      "    accuracy                         0.6809     16423\n",
      "   macro avg     0.5734    0.8849    0.5857     16423\n",
      "weighted avg     0.9456    0.6809    0.7640     16423\n",
      "\n",
      "\n",
      "Final Test Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.1272    0.9583    0.2246       120\n",
      "           1     0.9967    0.6191    0.7638     13595\n",
      "           2     0.9662    0.9934    0.9796      1207\n",
      "           3     0.0914    0.9400    0.1666       417\n",
      "           4     0.6731    0.9585    0.7909      1085\n",
      "\n",
      "    accuracy                         0.6797     16424\n",
      "   macro avg     0.5709    0.8939    0.5851     16424\n",
      "weighted avg     0.9437    0.6797    0.7623     16424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10)\n",
    "\n",
    "# Test evaluation\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        y_true.extend(y_batch.numpy())\n",
    "        y_pred.extend(preds.numpy())\n",
    "\n",
    "print(\"\\nFinal Test Performance:\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
